{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tjtyler/MachLearn_DecisionTree_ID3/blob/main/lab_3_decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxnT6qYjmuAl"
      },
      "source": [
        "# Decision Tree Lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4PM4YMtmuAo"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats as st\n",
        "import math \n",
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q36PewWCLZ6",
        "outputId": "d922a7e9-a103-4989-e0ec-25d5a6844e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR8Tao6WmuAq"
      },
      "source": [
        "## 1. (40%) Correctly implement the ID3 decision tree algorithm, including the ability to handle unknown attributes (You do not need to handle real valued attributes).  \n",
        "### Code Requirements/Notes:\n",
        "- Use standard information gain as your basic attribute evaluation metric.  (Note that normal ID3 would usually augment information gain with gain ratio or some other mechanism to penalize statistically insignificant attribute splits.) \n",
        "- You are welcome to create other classes and/or functions in addition to the ones provided below. (e.g. If you build out a tree structure, you might create a node class).\n",
        "- It is a good idea to use a simple data set (like the lenses data or the pizza homework), which you can check by hand, to test your algorithm to make sure that it is working correctly. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Node():\n",
        "  def __init__(self,X,y,par_node=None, par_entrop=None, node_feat=None, leaf=False,split_indx=None, depth=0):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.entropy = self.entropy()\n",
        "    self.parent_entropy = par_entrop\n",
        "    self.parent_node = par_node\n",
        "    self.children = None\n",
        "    self.leaf = leaf\n",
        "    self.depth = depth\n",
        "    self.split_indx = split_indx #index of attrib to split on\n",
        "    self.maj_class = None\n",
        "    self.info_gained = None\n",
        "    self.counts = self.calc_counts()\n",
        "    self.node_feat = node_feat\n",
        "\n",
        "  def entropy(self):\n",
        "    quants = np.bincount(self.y.flatten())\n",
        "    props = quants / np.sum(quants)\n",
        "    entrop = 0\n",
        "    for i in range(len(quants)):\n",
        "      if props[i] != 0:\n",
        "        entrop += -props[i]*math.log2(props[i])\n",
        "    return entrop\n",
        "  \n",
        "  def calc_counts(self):\n",
        "    xy_concat = np.concatenate((self.X, self.y.reshape(self.y.shape[0],1)), axis=1)\n",
        "    cnts = []\n",
        "    for i in range(xy_concat.shape[1]):\n",
        "      cnts.append(np.bincount(xy_concat[:,i]).shape[0])\n",
        "    return cnts\n",
        "\n",
        "\n",
        "  def set_split_indx(self, split_indx):\n",
        "    self.split_indx = split_indx\n",
        "  \n",
        "  def set_children(self, children):\n",
        "    self.children = children\n",
        "\n",
        "  def set_is_leaf(self,leaf):\n",
        "    self.leaf = leaf\n",
        "  \n",
        "  def set_majority_cls(self, cls):\n",
        "    self.maj_class = cls\n",
        "\n",
        "  def set_info_gained(self,info_gained):\n",
        "    self.info_gained = info_gained\n"
      ],
      "metadata": {
        "id": "71RmNY3yUDUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZntcQT9muAr"
      },
      "outputs": [],
      "source": [
        "class DTClassifier(BaseEstimator,ClassifierMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" Initialize class with chosen hyperparameters.\n",
        "        Args:\n",
        "        Optional Args (Args we think will make your life easier):\n",
        "            counts: A list of Ints that tell you how many types of each feature there are\n",
        "        Example:\n",
        "            DT  = DTClassifier()\n",
        "            or\n",
        "            DT = DTClassifier(count = [2,3,2,2])\n",
        "            Dataset = \n",
        "            [[0,1,0,0],\n",
        "            [1,2,1,1],\n",
        "            [0,1,1,0],\n",
        "            [1,2,0,1],\n",
        "            [0,0,1,1]]\n",
        "\n",
        "        \"\"\"\n",
        "        self.information_gained = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit the data; Make the Decision tree\n",
        "\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "            y (array-like): A 1D numpy array with the training targets\n",
        "\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "\n",
        "        \"\"\"\n",
        "        # self.root = Node()\n",
        "        # self.make_tree(self.root)\n",
        "        self.root = Node(X=X,y=y,par_node=None, par_entrop=None, node_feat=None, leaf=False,split_indx=None,depth=0)\n",
        "        self.make_tree(self.root)\n",
        "        return self\n",
        "\n",
        "    def make_tree(self,node):\n",
        "      # IF BASE CASE\n",
        "      # Base Case:\n",
        "        # if it's supposed to be a leaf node, make it a leaf node and set its majority class\n",
        "        # return\n",
        "      if node.entropy == 0 and node.y.shape[0] !=0:\n",
        "        node.set_is_leaf(True)\n",
        "        cls = node.y[0]\n",
        "        node.set_majority_cls(cls)\n",
        "        node.set_info_gained(node.entropy)\n",
        "        return\n",
        "\n",
        "      # IF THE NODE IS NOT A LEAF NODE THEN: \n",
        "        # DO STUFF\n",
        "        # Finding the best feature to split on\n",
        "        # Make the split (create all the children nodes) \n",
        "      else:\n",
        "        # FIND THE BEST FEATURE TO SPLIT ON\n",
        "        pos_children = [] # possible features to become children [[]]\n",
        "        avrg_children_ents = []\n",
        "\n",
        "        for ft_indx in range(node.X.shape[1]): # these are the columns of the X array\n",
        "          feat_cnts = np.bincount(node.X[:,ft_indx]) # this is a 1D np array that contains at each indx of the np.arr how many numbers are in in the column\n",
        "          feat_props = feat_cnts / np.sum(feat_cnts) # this is the proportions of each attribute of feat[ft_indx]\n",
        "          feat_children = [] # this will be a list of the children nodes of feat[ft_indx]\n",
        "          avrg_ent = 0 # average entropy of a feature's children\n",
        "          # LOOP OVER ALL OF THE ATTRIBUTES OF THE FEATURE (if counts = [3,4,3] and the feature is 0, then we loop from 0 to 2)\n",
        "          for unq_val in range(node.counts[ft_indx]):\n",
        "            xy_concat = np.concatenate((node.X, node.y.reshape(node.y.shape[0],1)), axis=1) # concatenate the X and y from the node that was passed in\n",
        "            Xy_child = xy_concat[xy_concat[:,ft_indx] == unq_val] # create the sub array for this child where the attribs of the feat = unq_val\n",
        "            child_X = Xy_child[:,:-1] # split Xy_child to get the X for the child\n",
        "            child_y = Xy_child[:,-1] # split Xy_child to get teh y for the child\n",
        "            child = Node(X=child_X,y=child_y,par_node=node, par_entrop=node.parent_entropy, node_feat=unq_val, depth=node.depth+1) # create the child node and increment its depth\n",
        "            avrg_ent += child.entropy*feat_props[unq_val] # add this child's entropy to the avrg entropy for this feature\n",
        "            feat_children.append(child) # add child to this features list of children\n",
        "          pos_children.append(feat_children) # add the list of children of the feature just condidered to the list of possible children\n",
        "          avrg_children_ents.append(avrg_ent) # add the avrg entropy of the children of the feature just considered to the lsit of average entropies\n",
        "\n",
        "        split_feat_indx = avrg_children_ents.index(min(avrg_children_ents)) # feature to split on wlll be the feature with the lowest avrg entropy\n",
        "        children_nodes = pos_children[split_feat_indx] # the children nodes of the feature to split on become the children nodes to keep\n",
        "        \n",
        "        node.set_split_indx(split_feat_indx) # set the splt_indx of the original node passed in\n",
        "        node.set_children(children_nodes) # assign the children to the original node passed in\n",
        "        \n",
        "        node.set_info_gained(node.entropy - avrg_children_ents[split_feat_indx])\n",
        "        self.information_gained.append(node.info_gained)\n",
        "        \n",
        "        # RECURSE ON CHILDREN\n",
        "        for child in children_nodes:\n",
        "          self.make_tree(child)\n",
        "\n",
        "    def print_tree(self):\n",
        "      self.print_tree_helper(self.root)\n",
        "\n",
        "    def print_tree_helper(self, node):\n",
        "    \n",
        "      # either print for a split node or print for a leaf node\n",
        "      if node.leaf:\n",
        "        print('\\t'*node.depth + f'prediction = {node.maj_class}')\n",
        "      # call print_tree on each child node\n",
        "      # ROOT NODE\n",
        "      else:\n",
        "        for child in node.children:\n",
        "          print('\\t'*node.depth + f'feature {node.split_indx} = ' + f'{child.node_feat}')\n",
        "          self.print_tree_helper(child)\n",
        "          \n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Predict all classes for a dataset X\n",
        "\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "\n",
        "        Returns:\n",
        "            array, shape (n_samples,)\n",
        "                Predicted target values per element in X.\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        # LOOP OVER THE INSTANCES OF THE DATA\n",
        "        for i in range(X.shape[0]):\n",
        "          inst = X[i]       \n",
        "          cur_node = self.root\n",
        "          \n",
        "          while not cur_node.leaf :\n",
        "            child_indx = inst[cur_node.split_indx]\n",
        "            # print('child_indx ', child_indx)\n",
        "            # print('cur_node.children ', cur_node.children)\n",
        "            if child_indx not in range(len(cur_node.children)):\n",
        "              cur_node = cur_node.parent_node\n",
        "              break\n",
        "            cur_node = cur_node.children[child_indx]\n",
        "          predictions.append(np.unique(cur_node.y)[0])\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    def score(self, X, y, shuffle=True):\n",
        "        \"\"\" Return accuracy(Classification Acc) of model on a given dataset. Must implement own score function.\n",
        "\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with data, excluding targets\n",
        "            y (array-like): A 1D numpy array of the targets \n",
        "        \"\"\"\n",
        "        if shuffle:\n",
        "          X,y = self._shuffle_data(X,y)\n",
        "        predictions = self.predict(X)\n",
        "        correct = 0\n",
        "        total = y.shape[0]\n",
        "        for i in range(0, y.shape[0]):\n",
        "          if predictions[i] == y[i][0]:\n",
        "            correct += 1\n",
        "        return correct/total\n",
        "\n",
        "    def _shuffle_data(self, X, y):\n",
        "      \"\"\" \n",
        "          Shuffle the data! This _ prefix suggests that this method should \n",
        "          only be called internally.\n",
        "          It might be easier to concatenate X & y and shuffle a single 2D \n",
        "          array, rather than shuffling X and y exactly the same way, \n",
        "          independently.\n",
        "      \"\"\"\n",
        "      single_arr = np.concatenate((X,y), axis=1) # concatenate X and y into a single array\n",
        "      np.random.shuffle(single_arr) # shuffle the rows of the concatenated X-y array\n",
        "      cutoff = single_arr.shape[1] - 1 # the point to split the X and y arrays after shuffling\n",
        "      X = single_arr[:,:cutoff] # the shuffled X array\n",
        "      y = single_arr[:,cutoff:] # the shuffled y array\n",
        "      return X,y\n",
        "\n",
        "    def get_information_gained(self):\n",
        "      return self.information_gained"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DEBUG HOMEWORK PIZZA PROBLEM\n",
        "\n",
        "HW_data = np.array([['Y', 'Thin', 'N', 'Great'],\n",
        "                    ['N','Deep', 'N', 'Bad'],\n",
        "                    ['N', 'Stuffed', 'Y','Good'],\n",
        "                    ['Y', 'Stuffed','Y','Great'],\n",
        "                    ['Y','Deep','N','Good'],\n",
        "                    ['Y','Deep','Y','Great'],\n",
        "                    ['N','Thin','Y','Good'],\n",
        "                    ['Y','Deep','N','Good'],\n",
        "                    ['N','Thin','N','Bad']])\n",
        "# N = 0; Y = 1; \n",
        "# Deep = 0; Stuffed = 1; Thin = 2\n",
        "# Bad = 0; Good = 1; Great = 2\n",
        "\n",
        "HW_data[HW_data == 'Y'] = 1\n",
        "HW_data[HW_data == 'N'] = 0\n",
        "HW_data[HW_data == 'Deep'] = 0\n",
        "HW_data[HW_data == 'Stuffed'] = 1\n",
        "HW_data[HW_data == 'Thin'] = 2\n",
        "HW_data[HW_data == 'Bad'] = 0\n",
        "HW_data[HW_data == 'Good'] = 1\n",
        "HW_data[HW_data == 'Great'] = 2\n",
        "HW_data = HW_data.astype(int)\n",
        "X = HW_data[:,:-1]\n",
        "y = HW_data[:,-1:]\n",
        "\n",
        "decision_tree = DTClassifier()\n",
        "decision_tree.fit(X,y)\n",
        "score = decision_tree.score(X=X,y=y)\n",
        "print('score ', score)\n",
        "decision_tree.print_tree()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbpgm49IQu-7",
        "outputId": "b43796de-e705-4dd4-fbc8-45190e75bd9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score  1.0\n",
            "feature 0 = 0\n",
            "\tfeature 2 = 0\n",
            "\t\tprediction = 0\n",
            "\tfeature 2 = 1\n",
            "\t\tprediction = 1\n",
            "feature 0 = 1\n",
            "\tfeature 1 = 0\n",
            "\t\tfeature 2 = 0\n",
            "\t\t\tprediction = 1\n",
            "\t\tfeature 2 = 1\n",
            "\t\t\tprediction = 2\n",
            "\tfeature 1 = 1\n",
            "\t\tprediction = 2\n",
            "\tfeature 1 = 2\n",
            "\t\tprediction = 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNKT1oNUmuAs"
      },
      "source": [
        "## 1.1 Debug\n",
        "\n",
        "Debug your model by training on the lenses dataset: [Debug Dataset (lenses.arff)](https://byu.instructure.com/courses/14142/files?preview=4622251)\n",
        "\n",
        "Test your model on the lenses test set: [Debug Test Dataset (lenses_test.arff)](https://byu.instructure.com/courses/14142/files?preview=4622254)\n",
        "\n",
        "Parameters:\n",
        "(optional) counts = [3,2,2,2] (You should compute this when you read in the data, before fitting)\n",
        "\n",
        "---\n",
        "\n",
        "Expected Results: Accuracy = [0.33]\n",
        "\n",
        "Predictions should match this file: [Lenses Predictions (pred_lenses.csv)](https://byu.instructure.com/courses/14142/files?preview=4622260)\n",
        "\n",
        "*NOTE: The [Lenses Prediction (pred_lenses.csv)](https://byu.instructure.com/courses/14142/files?preview=4622260) uses the following encoding: soft=2, hard=0, none=1. If your encoding is different, then your output will be different, but not necessarily incorrect.*\n",
        "\n",
        "Split Information Gains (These do not need to be in this exact order):\n",
        "\n",
        "[0.5487949406953987, 0.7704260414863775, 0.3166890883150208, 1.0, 0.4591479170272447, 0.9182958340544894]\n",
        "\n",
        "<!-- You should be able to get about 68% (61%-82%) predictive accuracy on the lenses data -->\n",
        "\n",
        "Here's what your decision tree splits should look like, and the corresponding child node predictions:\n",
        "\n",
        "Decision Tree:\n",
        "<pre>\n",
        "tear_prod_rate = normal:\n",
        "    astigmatism = no:\n",
        "        age = pre_presbyopic:\n",
        "            prediction: soft\n",
        "        age = presbyopic:\n",
        "            spectacle_prescrip = hypermetrope:\n",
        "                prediction: soft\n",
        "            spectacle_prescrip = myope:\n",
        "                prediction: none\n",
        "        age = young:\n",
        "            prediction: soft\n",
        "    astigmatism = yes:\n",
        "        spectacle_prescrip = hypermetrope:\n",
        "            age = pre_presbyopic:\n",
        "                prediction: none\n",
        "            age = presbyopic:\n",
        "                prediction: none\n",
        "            age = young:\n",
        "                prediction: hard\n",
        "        spectacle_prescrip = myope:\n",
        "            prediction: hard\n",
        "tear_prod_rate = reduced:\n",
        "    prediction: none\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5WsJACLmuAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b535d151-5a23-4591-fdb9-10eb493ec198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.3333333333333333\n",
            "information gained:  [0.5487949406953982, 0.7704260414863778, 0.3166890883150208, 1.0, 0.4591479170272448, 0.9182958340544896]\n"
          ]
        }
      ],
      "source": [
        "from scipy.io.arff import loadarff \n",
        "import pandas as pd\n",
        "\n",
        "# Load debug training data\n",
        "raw_data = loadarff('/content/drive/MyDrive/School/CS_472_MachLearning/labs/lab3_decisionTree/data/lenses.arff')\n",
        "df_data = pd.DataFrame(raw_data[0])\n",
        "\n",
        "np_arr = df_data.to_numpy() #cast dataframe to numpy array\n",
        "np_arr[np_arr == b'young'] = 0 \n",
        "np_arr[np_arr == b'pre_presbyopic'] = 1 \n",
        "np_arr[np_arr == b'presbyopic'] = 2 \n",
        "\n",
        "np_arr[np_arr == b'myope'] = 0 \n",
        "np_arr[np_arr == b'hypermetrope'] = 1 \n",
        "\n",
        "np_arr[np_arr == b'no'] = 0\n",
        "np_arr[np_arr == b'yes'] = 1\n",
        "\n",
        "np_arr[np_arr == b'reduced'] = 0\n",
        "np_arr[np_arr == b'normal'] = 1\n",
        "\n",
        "np_arr[np_arr == b'soft'] = 0\n",
        "np_arr[np_arr == b'hard'] = 1\n",
        "np_arr[np_arr == b'none'] = 2\n",
        "\n",
        "np_arr = np_arr.astype(int)\n",
        "\n",
        "training_data = np_arr.copy()\n",
        "\n",
        "X_train = training_data[:,:-1]\n",
        "y_train = training_data[:,-1:]\n",
        "\n",
        "# Train Decision Tree\n",
        "decision_tree = DTClassifier()\n",
        "decision_tree.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "# Load debug test data\n",
        "raw_data = loadarff('/content/drive/MyDrive/School/CS_472_MachLearning/labs/lab3_decisionTree/data/lenses_test.arff')\n",
        "df_data = pd.DataFrame(raw_data[0])\n",
        "\n",
        "np_arr = df_data.to_numpy() #cast dataframe to numpy array\n",
        "np_arr[np_arr == b'young'] = 0 \n",
        "np_arr[np_arr == b'pre_presbyopic'] = 1 \n",
        "np_arr[np_arr == b'presbyopic'] = 2 \n",
        "\n",
        "np_arr[np_arr == b'myope'] = 0 \n",
        "np_arr[np_arr == b'hypermetrope'] = 1 \n",
        "\n",
        "np_arr[np_arr == b'no'] = 0\n",
        "np_arr[np_arr == b'yes'] = 1\n",
        "\n",
        "np_arr[np_arr == b'reduced'] = 0\n",
        "np_arr[np_arr == b'normal'] = 1\n",
        "\n",
        "np_arr[np_arr == b'soft'] = 0\n",
        "np_arr[np_arr == b'hard'] = 1\n",
        "np_arr[np_arr == b'none'] = 2\n",
        "\n",
        "np_arr = np_arr.astype(int)\n",
        "\n",
        "test_data = np_arr.copy()\n",
        "\n",
        "X_test = test_data[:,:-1]\n",
        "y_test = test_data[:,-1:]\n",
        "\n",
        "# Predict and compute model accuracy\n",
        "score = decision_tree.score(X=X_test,y=y_test)\n",
        "print('Accuracy: ', score)\n",
        "\n",
        "# Print the information gain of every split you make.\n",
        "print('information gained: ', decision_tree.get_information_gained())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgUPhI6bmuAt"
      },
      "outputs": [],
      "source": [
        "# Optional/Additional Debugging Dataset - Pizza Homework\n",
        "# pizza_dataset = np.array([[1,2,0],[0,0,0],[0,1,1],[1,1,1],[1,0,0],[1,0,1],[0,2,1],[1,0,0],[0,2,0]])\n",
        "# pizza_labels = np.array([2,0,1,2,1,2,1,1,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMmh1D0PmuAu"
      },
      "source": [
        "## 1.2 Evaluation\n",
        "\n",
        "We will evaluate your model based on its performance on the zoo dataset. \n",
        "\n",
        "Train your model using this dataset: [Evaluation Train Dataset (zoo.arff)](https://byu.instructure.com/courses/14142/files?preview=4622270)\n",
        "\n",
        "Test your model on this dataset: [Evaluation Test Dataset (zoo_test.arff)](https://byu.instructure.com/courses/14142/files?preview=4622274)\n",
        "\n",
        "Parameters:\n",
        "(optional) counts = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2] (You should compute this when you read in the data, before fitting)\n",
        "\n",
        "---\n",
        "Print out your accuracy on the evaluation test dataset.\n",
        "\n",
        "Print out the information gain of every split you make."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTkt1squmuAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b37ecf-8fe1-4c65-fb97-7205a02e3660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.147\n",
            "information gained:  [1.3630469031539394, 0.8865408928220899, 0.9852281360342515, 0.6962122601251459, 0, 0.8256265261578954, 0, 0.6892019851173656, 0.8631205685666308, 0.7219280948873623, 0.7219280948873623, 0]\n"
          ]
        }
      ],
      "source": [
        "from scipy.io.arff import loadarff \n",
        "import pandas as pd\n",
        "\n",
        "# Load debug training data\n",
        "raw_data = loadarff('/content/drive/MyDrive/School/CS_472_MachLearning/labs/lab3_decisionTree/data/zoo.arff')\n",
        "df_data = pd.DataFrame(raw_data[0])\n",
        "\n",
        "np_arr = df_data.to_numpy() #cast dataframe to numpy array\n",
        "np_arr[np_arr == b'F'] = 0 \n",
        "np_arr[np_arr == b'T'] = 1 \n",
        "\n",
        "np_arr[np_arr == b'cT'] = 0 \n",
        "np_arr[np_arr == b'c2'] = 1 \n",
        "np_arr[np_arr == b'c3'] = 2\n",
        "np_arr[np_arr == b'c4'] = 3\n",
        "np_arr[np_arr == b'c5'] = 4\n",
        "np_arr[np_arr == b'c6'] = 5\n",
        "np_arr[np_arr == b'c7'] = 6\n",
        "\n",
        "np_arr = np_arr.astype(int)\n",
        "\n",
        "training_data = np_arr.copy()\n",
        "\n",
        "X_train = training_data[:,:-1]\n",
        "y_train = training_data[:,-1:]\n",
        "\n",
        "# Train Decision Tree\n",
        "decision_tree = DTClassifier()\n",
        "decision_tree.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "# Load debug test data\n",
        "raw_data = loadarff('/content/drive/MyDrive/School/CS_472_MachLearning/labs/lab3_decisionTree/data/zoo_test.arff')\n",
        "df_data = pd.DataFrame(raw_data[0])\n",
        "\n",
        "np_arr = df_data.to_numpy() #cast dataframe to numpy array\n",
        "np_arr[np_arr == b'F'] = 0 \n",
        "np_arr[np_arr == b'T'] = 1 \n",
        "\n",
        "np_arr[np_arr == b'cT'] = 0 \n",
        "np_arr[np_arr == b'c2'] = 1 \n",
        "np_arr[np_arr == b'c3'] = 2\n",
        "np_arr[np_arr == b'c4'] = 3\n",
        "np_arr[np_arr == b'c5'] = 4\n",
        "np_arr[np_arr == b'c6'] = 5\n",
        "np_arr[np_arr == b'c7'] = 6\n",
        "\n",
        "np_arr = np_arr.astype(int)\n",
        "\n",
        "test_data = np_arr.copy()\n",
        "\n",
        "X_test = test_data[:,:-1]\n",
        "y_test = test_data[:,-1:]\n",
        "\n",
        "# Predict and compute model accuracy\n",
        "score = decision_tree.score(X=X_test,y=y_test)\n",
        "print('Accuracy: ', score)\n",
        "\n",
        "# Print the information gain of every split you make.\n",
        "print('information gained: ', decision_tree.get_information_gained())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8edh2HFWmuAv"
      },
      "source": [
        "## 2. (20%) You will use your ID3 algorithm to induce decision trees for the cars dataset and the voting dataset.  Do not use a stopping criterion, but induce the tree as far as it can go (until classes are pure or there are no more data or attributes to split on).  \n",
        "- Implement and use 10-fold Cross Validation (CV) on each data set to predict how well the models will do on novel data.  \n",
        "- For each dataset, report the training and test classification accuracy for each fold and the average test accuracy. \n",
        "- As a rough sanity check, typical decision tree accuracies for these data sets are: Cars: .90-.95, Vote: .92-.95."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjLh24oDmuAw"
      },
      "source": [
        "## 2.1 Implement 10-fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold_cv(arr, k):\n",
        "  n_samples = arr.shape[0]\n",
        "  \n",
        "  test_size = int((n_samples)/k)\n",
        "\n",
        "  train_accuracies = []\n",
        "  test_accuracies = []\n",
        "  for i in range(k):\n",
        "    start_indx = i*test_size\n",
        "    end_indx = (i+1)*test_size\n",
        "\n",
        "    sub_arrays = np.split(arr,(start_indx, end_indx), axis=0)\n",
        "\n",
        "    train = np.concatenate((sub_arrays[0],sub_arrays[2]), axis=0)\n",
        "    X_train = train[:,:-1]\n",
        "    y_train = train[:,-1:]\n",
        "    # y_train = y_train.reshape(y_train.shape[0],1)\n",
        "\n",
        "    test = sub_arrays[1]\n",
        "    X_test = test[:,:-1]\n",
        "    y_test = test[:,-1:]\n",
        "    # y_test = y_test.reshape(y_test.shape[0],1)\n",
        "    \n",
        "    DT_cls = DTClassifier()\n",
        "    DT_cls.fit(X_train,y_train)\n",
        "\n",
        "    train_score = DT_cls.score(X_train,y_train)\n",
        "    test_score = DT_cls.score(X_test, y_test)\n",
        "\n",
        "    train_accuracies.append(train_score)\n",
        "    test_accuracies.append(test_score)\n",
        "\n",
        "    if i == 9:\n",
        "      DT_cls.print_tree()\n",
        "\n",
        "  avrg_train_accur = sum(train_accuracies) / len(train_accuracies)\n",
        "  avrg_test_accur = sum(test_accuracies) / len(test_accuracies)\n",
        "\n",
        "  return avrg_train_accur, avrg_test_accur, train_accuracies, test_accuracies\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ToOHvX1d5XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnGxm88EmuAw"
      },
      "source": [
        "##  2.2 Cars Dataset\n",
        "- Use this [Cars Dataset (cars.arff)](hhttps://byu.instructure.com/courses/14142/files?preview=4622293)\n",
        "- Make a table for your k-fold cross validation accuracies\n",
        "\n",
        "*If you are having trouble using scipy's loadarff function (scipy.io.arff.loadarff), try:*\n",
        "\n",
        "*pip install arff &nbsp;&nbsp;&nbsp;&nbsp;          # Install arff library*\n",
        "\n",
        "*import arff as arf*                   \n",
        "\n",
        "*cars = list(arf.load('cars.arff'))   &nbsp;&nbsp;&nbsp;&nbsp;# Load your downloaded dataset (!curl, etc.)*\n",
        "\n",
        "*df = pd.DataFrame(cars)*  \n",
        "\n",
        "*There may be additional cleaning needed*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WQTa7AecmuAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40272c99-4610-4e30-cac6-26ca92b4a407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 3 0 0 2 1 2]\n",
            "[3 3 0 0 2 2 2]\n",
            "[3 3 0 0 2 0 2]\n",
            "feature 5 = 0\n",
            "\tfeature 3 = 0\n",
            "\t\tprediction = 2\n",
            "\tfeature 3 = 1\n",
            "\t\tfeature 0 = 0\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tprediction = 2\n",
            "\t\tfeature 0 = 1\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 1\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 1\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tprediction = 0\n",
            "\t\tfeature 0 = 2\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 1\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tprediction = 0\n",
            "\t\tfeature 0 = 3\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tprediction = 2\n",
            "\tfeature 3 = 2\n",
            "\t\tfeature 0 = 0\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tprediction = 2\n",
            "\t\tfeature 0 = 1\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 1\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\tfeature 0 = 2\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 3\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\tfeature 0 = 3\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tprediction = 2\n",
            "feature 5 = 1\n",
            "\tprediction = 2\n",
            "feature 5 = 2\n",
            "\tfeature 3 = 0\n",
            "\t\tprediction = 2\n",
            "\tfeature 3 = 1\n",
            "\t\tfeature 0 = 0\n",
            "\t\t\tfeature 4 = 0\n",
            "\t\t\t\tfeature 1 = 0\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 1 = 1\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 1 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 1 = 3\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 4 = 1\n",
            "\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\tfeature 1 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 1 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 1 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 1 = 3\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\tfeature 1 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 1 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 1 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 1 = 3\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 4 = 2\n",
            "\t\t\t\tprediction = 2\n",
            "\t\tfeature 0 = 1\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\tfeature 0 = 2\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\tfeature 0 = 3\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tprediction = 2\n",
            "\tfeature 3 = 2\n",
            "\t\tfeature 0 = 0\n",
            "\t\t\tfeature 4 = 0\n",
            "\t\t\t\tfeature 1 = 0\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 1 = 1\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 1 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 1 = 3\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 4 = 1\n",
            "\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\tfeature 1 = 0\n",
            "\t\t\t\t\tfeature 1 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 1 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 1 = 3\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\tfeature 1 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 1 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 1 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 1 = 3\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 4 = 2\n",
            "\t\t\t\tprediction = 2\n",
            "\t\tfeature 0 = 1\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\tfeature 0 = 2\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\tfeature 0 = 3\n",
            "\t\t\tfeature 1 = 0\n",
            "\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 1 = 1\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 3\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 1 = 2\n",
            "\t\t\t\tfeature 4 = 0\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 1\n",
            "\t\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\t\t\tprediction = 2\n",
            "\t\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 4 = 2\n",
            "\t\t\t\t\tprediction = 2\n",
            "\t\t\tfeature 1 = 3\n",
            "\t\t\t\tprediction = 2\n",
            "Training Accuracies: \n",
            " [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Test Accuracies: \n",
            " [0.936046511627907, 0.9709302325581395, 0.9593023255813954, 0.936046511627907, 0.9534883720930233, 0.936046511627907, 0.9651162790697675, 0.9302325581395349, 0.9011627906976745, 0.9651162790697675]\n",
            "Average Train Accuracy:  1.0\n",
            "Average Test Accuracy:  0.9453488372093023\n",
            "[array([b'high', b'low', b'med', b'vhigh'], dtype=object), array([b'high', b'low', b'med', b'vhigh'], dtype=object), array([b'2', b'3', b'4', b'5more'], dtype=object), array([b'2', b'4', b'more'], dtype=object), array([b'big', b'med', b'small'], dtype=object), array([b'high', b'low', b'med'], dtype=object), array([b'acc', b'good', b'unacc', b'vgood'], dtype=object)]\n"
          ]
        }
      ],
      "source": [
        "from scipy.io.arff import loadarff \n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Load debug training data\n",
        "raw_data = loadarff('/content/drive/MyDrive/School/CS_472_MachLearning/labs/lab3_decisionTree/data/cars.arff')\n",
        "df_data = pd.DataFrame(raw_data[0])\n",
        "\n",
        "Oec = OrdinalEncoder()\n",
        "np_arr = Oec.fit_transform(df_data).astype(int)\n",
        "print(np_arr[0])\n",
        "print(np_arr[1])\n",
        "print(np_arr[2])\n",
        "# SHUFFLE DATA\n",
        "np.random.shuffle(np_arr)\n",
        "\n",
        "# Use 10-fold CV on Cars Dataset\n",
        "avrg_train_accur, avrg_test_accur, train_accuracies, test_accuracies = k_fold_cv(np_arr,10)\n",
        "# Report Training and Test Classification Accuracies\n",
        "print('Training Accuracies: \\n', train_accuracies)\n",
        "print('Test Accuracies: \\n', test_accuracies)\n",
        "# Report Average Test Accuracy\n",
        "print('Average Train Accuracy: ', avrg_train_accur)\n",
        "print('Average Test Accuracy: ', avrg_test_accur)\n",
        "\n",
        "print(Oec.categories_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9ul48zkmuAx"
      },
      "source": [
        "## 2.3 Voting Dataset\n",
        "- Use this [Voting Dataset with missing values (voting_with_missing.arff)](https://byu.instructure.com/courses/14142/files?preview=4622298)\n",
        "- Note that you will need to support unknown attributes in the voting data set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmWdEbGxmuAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09013f94-1124-46c5-9449-64928bce6dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature 3 = 0\n",
            "\tfeature 8 = 0\n",
            "\t\tprediction = 1\n",
            "\tfeature 8 = 1\n",
            "\t\tprediction = 0\n",
            "\tfeature 8 = 2\n",
            "\t\tfeature 6 = 0\n",
            "\t\t\tprediction = 0\n",
            "\t\tfeature 6 = 1\n",
            "\t\t\tprediction = 1\n",
            "\t\tfeature 6 = 2\n",
            "\t\t\tprediction = 0\n",
            "feature 3 = 1\n",
            "\tfeature 2 = 0\n",
            "\t\tprediction = 0\n",
            "\tfeature 2 = 1\n",
            "\t\tfeature 11 = 0\n",
            "\t\t\tprediction = 1\n",
            "\t\tfeature 11 = 1\n",
            "\t\t\tfeature 10 = 0\n",
            "\t\t\tfeature 10 = 1\n",
            "\t\t\t\tfeature 5 = 0\n",
            "\t\t\t\tfeature 5 = 1\n",
            "\t\t\t\t\tfeature 13 = 0\n",
            "\t\t\t\t\tfeature 13 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 13 = 2\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 5 = 2\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 10 = 2\n",
            "\t\t\t\tprediction = 0\n",
            "\t\tfeature 11 = 2\n",
            "\t\t\tprediction = 0\n",
            "\tfeature 2 = 2\n",
            "\t\tprediction = 0\n",
            "feature 3 = 2\n",
            "\tfeature 10 = 0\n",
            "\t\tprediction = 1\n",
            "\tfeature 10 = 1\n",
            "\t\tfeature 14 = 0\n",
            "\t\t\tprediction = 1\n",
            "\t\tfeature 14 = 1\n",
            "\t\t\tfeature 15 = 0\n",
            "\t\t\t\tfeature 2 = 0\n",
            "\t\t\t\tfeature 2 = 1\n",
            "\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 2 = 2\n",
            "\t\t\t\t\tfeature 0 = 0\n",
            "\t\t\t\t\tfeature 0 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 0 = 2\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\tfeature 15 = 1\n",
            "\t\t\t\tprediction = 1\n",
            "\t\t\tfeature 15 = 2\n",
            "\t\t\t\tprediction = 1\n",
            "\t\tfeature 14 = 2\n",
            "\t\t\tfeature 9 = 0\n",
            "\t\t\tfeature 9 = 1\n",
            "\t\t\t\tfeature 11 = 0\n",
            "\t\t\t\tfeature 11 = 1\n",
            "\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 11 = 2\n",
            "\t\t\t\t\tprediction = 1\n",
            "\t\t\tfeature 9 = 2\n",
            "\t\t\t\tprediction = 1\n",
            "\tfeature 10 = 2\n",
            "\t\tfeature 2 = 0\n",
            "\t\t\tprediction = 0\n",
            "\t\tfeature 2 = 1\n",
            "\t\t\tfeature 4 = 0\n",
            "\t\t\tfeature 4 = 1\n",
            "\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 4 = 2\n",
            "\t\t\t\tfeature 15 = 0\n",
            "\t\t\t\t\tfeature 0 = 0\n",
            "\t\t\t\t\tfeature 0 = 1\n",
            "\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\tfeature 0 = 2\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\tfeature 15 = 1\n",
            "\t\t\t\t\tfeature 12 = 0\n",
            "\t\t\t\t\tfeature 12 = 1\n",
            "\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\tfeature 12 = 2\n",
            "\t\t\t\t\t\tfeature 1 = 0\n",
            "\t\t\t\t\t\tfeature 1 = 1\n",
            "\t\t\t\t\t\t\tfeature 0 = 0\n",
            "\t\t\t\t\t\t\tfeature 0 = 1\n",
            "\t\t\t\t\t\t\t\tprediction = 0\n",
            "\t\t\t\t\t\t\tfeature 0 = 2\n",
            "\t\t\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\t\t\tfeature 1 = 2\n",
            "\t\t\t\t\t\t\tprediction = 1\n",
            "\t\t\t\tfeature 15 = 2\n",
            "\t\t\t\t\tprediction = 1\n",
            "\t\tfeature 2 = 2\n",
            "\t\t\tfeature 6 = 0\n",
            "\t\t\tfeature 6 = 1\n",
            "\t\t\t\tprediction = 0\n",
            "\t\t\tfeature 6 = 2\n",
            "\t\t\t\tprediction = 1\n",
            "Training Accuracies: \n",
            " [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Test Accuracies: \n",
            " [0.9069767441860465, 0.9534883720930233, 0.9302325581395349, 0.9534883720930233, 0.9302325581395349, 0.9302325581395349, 0.9534883720930233, 0.9534883720930233, 0.9069767441860465, 0.9767441860465116]\n",
            "Average Train Accuracy:  1.0\n",
            "Average Test Accuracy:  0.9395348837209303\n",
            "[array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'?', b'n', b'y'], dtype=object), array([b'democrat', b'republican'], dtype=object)]\n"
          ]
        }
      ],
      "source": [
        "from scipy.io.arff import loadarff \n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Load debug training data\n",
        "raw_data = loadarff('/content/drive/MyDrive/School/CS_472_MachLearning/labs/lab3_decisionTree/data/voting_with_missing.arff')\n",
        "df_data = pd.DataFrame(raw_data[0])\n",
        "\n",
        "Oec = OrdinalEncoder()\n",
        "np_arr = Oec.fit_transform(df_data).astype(int)\n",
        "\n",
        "# SHUFFLE DATA\n",
        "np.random.shuffle(np_arr)\n",
        "\n",
        "# Use 10-fold CV on Cars Dataset\n",
        "avrg_train_accur, avrg_test_accur, train_accuracies, test_accuracies = k_fold_cv(np_arr,10)\n",
        "# Report Training and Test Classification Accuracies\n",
        "print('Training Accuracies: \\n', train_accuracies)\n",
        "print('Test Accuracies: \\n', test_accuracies)\n",
        "# Report Average Test Accuracy\n",
        "print('Average Train Accuracy: ', avrg_train_accur)\n",
        "print('Average Test Accuracy: ', avrg_test_accur)\n",
        "# Used 10-fold CV on Voting Dataset\n",
        "print(Oec.categories_)\n",
        "# Report Training and Test Classification Accuracies\n",
        "\n",
        "# Report Average Test Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8PraMbJmuAy"
      },
      "source": [
        "## 2.4 Discuss Your Results\n",
        "\n",
        "- Summarize your results from both datasets, and discuss what you observed. \n",
        "- A fully expanded tree will often get 100% accuracy on the training set. Why does this happen and in what cases might it not?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6poS45-CmuAy"
      },
      "source": [
        "Summary: on both the cars and voting data, my decision tree had similar results for the accuracies. The average training data accuracy for both datasets was 1.0 or 100%. the averages for the testing data were also similar, being about .94 or 94% for both datasets.\n",
        "\n",
        "The reason that a fully expanded tree will often get 100% accuracy on the training set is because this is the data that is used to build the tree, so as long as there is no data and corresponding classification that contradicts other classifications, the tree should be 100% accurate at classifying something that it has already seen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swx-3h5_muAy"
      },
      "source": [
        "## 3. (15%) For each of the two problems above, summarize in English what the decision tree has learned (i.e., look at the induced tree and describe what rules it has discovered to try to solve each task). \n",
        "- If the tree is very large you can just discuss a few of the more shallow attribute combinations and the most important decisions made high in the tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHrhN-sJmuAy"
      },
      "source": [
        "## 3.1 Discuss what the decision tree induced on the cars dataset has learned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbcT0lr7muAz"
      },
      "source": [
        "For the cars dataset, it found that when Safety was high and Persons was 2 that the prediction was unacceptable. it likewise found that  when Safety was high, Persons was 4, buying price was high and maintenance cost was low, medium, or high, the predicted condition was acceptable; if the maintenance cost was very high though, then the condition became unaccetable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJcvXPKJmuAz"
      },
      "source": [
        "## 3.2 Discuss what the decision tree induced on the voting dataset has learned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWflACnmuAz"
      },
      "source": [
        "For the voting dataset, it found that if someone did not vote on physician-fee-freeze and also did not vote on the mx-missile that they were predicted to be a republican. if they didn't vote on the the physician-fee-freeze and voted no on the mx-missile then they were predicted to be a democrat. if they voted no on the physician-fee-freeze and didn't vote on adoption-of-the-budget-resolution then they are predicted to be a democrat. And if they voted no on the physician-fee-freeze, no on the adoption-of-the-budget-resolution, and didn't vote on the education-spending then they are predicted to be a republican."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkkfBYODmuAz"
      },
      "source": [
        "## 3.3 How did you handle unknown attributes in the voting problem? Why did you choose this approach? (Do not use the approach of just throwing out data with unknown attributes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E-K0slxmuAz"
      },
      "source": [
        "To handle missing values in the voting dataset I chose to consider missing data as an additional attribute on top of the existing attributes for that feature. I chose this approach because in this case of voting in congress, not voting is a strategy that is often used by congressmen/congresswomen. Because of this there could be a lot of information contained in the fact that they he/she didn't vote on an item. This was also the easiest thing to do because I used OrdinalEncoder from sklearn which encodes the string values and missing values (?) as numbers, so I didn't have to do anything extra to preprocess the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGRFOWaymuA0"
      },
      "source": [
        "## 4.1 (10%) Use Scikit Learn's decision tree on the voting dataset and compare your results. Try different parameters and report what parameters perform best on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def getSplit(arr,dec):\n",
        "  \"\"\"\n",
        "  the 'arr' arguement should be the full numpy array before it is split into X and y\n",
        "  \"\"\"\n",
        "  # THE NAMES OF THESE VARIABLES ARE AS IF THE SPLIT IS 80/20 BUT IT WORKS FOR ANY PERCENT BASED ON DEC (which is a decimal value)\n",
        "  rand_start_index = random.randint(0,arr.shape[0]-1)\n",
        "  num_vals_80 = round((arr.shape[0])*dec)\n",
        "  num_vals_20 = arr.shape[0] - num_vals_80\n",
        "  arr_80_combined = None\n",
        "  arr_20 = None\n",
        "  if (rand_start_index + num_vals_80) >= arr.shape[0]: # if the starting index (row) + the number of rows we need to make 80% of the rows >= the number of rows in arr\n",
        "    num_vals_from_start = (rand_start_index + num_vals_80) - arr.shape[0] - 1 # the number of rows past the last row (back to the first row) that we need to make 80%\n",
        "    arr_80_pt1 = arr[:num_vals_from_start,:] # part1 of the 80%_array from row[0] to row[num_vals_from_start - 1]\n",
        "    arr_80_pt2 = arr[rand_start_index:,:] # part2 of the 80%_array from the rand_start_index (row) to the last index (row)\n",
        "    arr_80_combined = np.concatenate((arr_80_pt1,arr_80_pt2), axis=0)\n",
        "    arr_20 = arr[num_vals_from_start:rand_start_index,:] # 20%_array is everything in between\n",
        "  else:\n",
        "    arr_80_combined = arr[rand_start_index:num_vals_80,:]\n",
        "    arr_20 = arr[num_vals_80:,:]\n",
        "  return arr_80_combined, arr_20"
      ],
      "metadata": {
        "id": "RK-8ynKL4bhF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZSoe3KtmuA0"
      },
      "source": [
        "### 4.1.1 sklearn on Voting Dataset\n",
        "- Use this [Voting Dataset with missing values (voting_with_missing.arff)](https://byu.instructure.com/courses/14142/files?preview=4622298)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "W4c7V2YNmuA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef87144-f9b1-4cc4-e1f0-a62d1ad9fcaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for entropy/best_splitter:  0.9431818181818182\n",
            "Accuracy for gini/best_splitter:  0.9431818181818182\n",
            "Accuracy for entropy/random_splitter:  0.9431818181818182\n",
            "Accuracy for gini/random_splitter:  0.9090909090909091\n",
            "\n",
            "\n",
            "Accuracy for entropy/best_splitter/log2_max_feats:  0.8863636363636364\n",
            "Accuracy for entropy/best_splitter/4_min_samples_slit:  0.9431818181818182\n"
          ]
        }
      ],
      "source": [
        "from scipy.io.arff import loadarff \n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn import tree\n",
        "# Load debug training data\n",
        "raw_data = loadarff('/content/drive/MyDrive/School/CS_472_MachLearning/labs/lab3_decisionTree/data/voting_with_missing.arff')\n",
        "df_data = pd.DataFrame(raw_data[0])\n",
        "\n",
        "Oec = OrdinalEncoder()\n",
        "np_arr = Oec.fit_transform(df_data).astype(int)\n",
        "\n",
        "# SHUFFLE DATA\n",
        "np.random.shuffle(np_arr)\n",
        "train, test = getSplit(np_arr,.8)\n",
        "X_train = train[:,:-1]\n",
        "y_train = train[:,-1]\n",
        "X_test = test[:,:-1]\n",
        "y_test = test[:,-1]\n",
        "# Use sklearn's Decision Tree to learn the voting dataset\n",
        "\n",
        "# Explore different parameters\n",
        "\n",
        "# Report results\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\")\n",
        "clf = clf.fit(X_train, y_train)\n",
        "clf = clf.score(X_test,y_test)\n",
        "print('Accuracy for entropy/best_splitter: ', clf)\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\")\n",
        "clf = clf.fit(X_train, y_train)\n",
        "clf = clf.score(X_test,y_test)\n",
        "print('Accuracy for gini/best_splitter: ', clf)\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", splitter=\"random\")\n",
        "clf = clf.fit(X_train, y_train)\n",
        "clf = clf.score(X_test,y_test)\n",
        "print('Accuracy for entropy/random_splitter: ', clf)\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"gini\", splitter=\"random\")\n",
        "clf = clf.fit(X_train, y_train)\n",
        "clf = clf.score(X_test,y_test)\n",
        "print('Accuracy for gini/random_splitter: ', clf)\n",
        "\n",
        "print('\\n')\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", max_features=\"log2\")\n",
        "clf = clf.fit(X_train, y_train)\n",
        "clf = clf.score(X_test,y_test)\n",
        "print('Accuracy for entropy/best_splitter/log2_max_feats: ', clf)\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", min_samples_split=4)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "clf = clf.score(X_test,y_test)\n",
        "print('Accuracy for entropy/best_splitter/4_min_samples_slit: ', clf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3vIBR7ImuA0"
      },
      "source": [
        "##Discussion of Results:\n",
        "\n",
        "By doing multiple combinations of DecisionTreeClassifier parameter variations for criterion, splitter, max_features, and min_samples_split, I found that there is very little (if any) difference when criterion is set to \"entropy\" or \"gini\" and splitter is set to \"best\" or \"random\". There is a little bit of variation between the DT with max_features=\"log2\" and min_samples_split=4 but not a significant difference. \n",
        "\n",
        "The lack of variation is likely partly due to the size of the voting data set, with it only having about 400 instances. If there were more training instances and more features than these changes in parameters might actually affect the accuracy more than they do with this voting dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad8tZQhRmuA0"
      },
      "source": [
        "## 4.2 (10%) Choose a data set of your choice (not already used in this or previous labs) and use the sklearn decision tree to learn it. Experiment with different hyper-parameters to try to get the best results possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "Inl9skMNmuA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "033a0bec-8fb9-42bc-ad40-1c7a7b4edd74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for entropy/best_splitter:  0.43548387096774194\n",
            "Accuracy for gini/best_splitter:  0.5161290322580645\n",
            "Accuracy for entropy/best_splitter/4_max_depth:  0.5806451612903226\n",
            "Accuracy for entropy/best_splitter/.1_min_wt_frac_leaf:  0.6290322580645161\n"
          ]
        }
      ],
      "source": [
        "# Use sklearn's Decision Tree on a new dataset\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn import tree\n",
        "# Load debug training data\n",
        "# data is heart disease https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
        "df = pd.read_csv('/content/drive/MyDrive/School/CS_472_MachLearning/labs/lab3_decisionTree/data/processed.cleveland.data')\n",
        "\n",
        "np_arr = df.to_numpy() #cast dataframe to numpy array\n",
        "\n",
        "Oec = OrdinalEncoder()\n",
        "np_arr = Oec.fit_transform(np_arr)\n",
        "\n",
        "np_arr = np_arr.astype(int)\n",
        "\n",
        "# SHUFFLE DATA\n",
        "np.random.shuffle(np_arr)\n",
        "train, test = getSplit(np_arr,.8)\n",
        "X_train = train[:,:-1]\n",
        "y_train = train[:,-1]\n",
        "X_test = test[:,:-1]\n",
        "y_test = test[:,-1]\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\")\n",
        "clf = clf.fit(X_train, y_train)\n",
        "clf = clf.score(X_test,y_test)\n",
        "print('Accuracy for entropy/best_splitter: ', clf)\n",
        "\n",
        "# Experiment with different hyper-parameters\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\")\n",
        "clf = clf.fit(X_train, y_train)\n",
        "clf = clf.score(X_test,y_test)\n",
        "print('Accuracy for gini/best_splitter: ', clf)\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", max_depth=3)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "clf = clf.score(X_test,y_test)\n",
        "print('Accuracy for entropy/best_splitter/4_max_depth: ', clf)\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", min_weight_fraction_leaf=.1)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "clf = clf.score(X_test,y_test)\n",
        "print('Accuracy for entropy/best_splitter/.1_min_wt_frac_leaf: ', clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5O6u5DdmuA1"
      },
      "source": [
        "## 5. (5%) Visualize sklearn's decision tree for your chosen data set (using export_graphviz or another tool) and discuss what you find. If your tree is too deep to reasonably fit on one page, show only the first few levels (e.g., top 5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "P6-6es-pmuA1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "f19fa889-a413-4df3-9971-02d3ae18227c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.files.Source at 0x7f1af93fd890>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"890pt\" height=\"373pt\"\n viewBox=\"0.00 0.00 890.00 373.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 369)\">\n<title>Tree</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-369 886,-369 886,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#f4caab\" stroke=\"#000000\" d=\"M520.5,-365C520.5,-365 348.5,-365 348.5,-365 342.5,-365 336.5,-359 336.5,-353 336.5,-353 336.5,-309 336.5,-309 336.5,-303 342.5,-297 348.5,-297 348.5,-297 520.5,-297 520.5,-297 526.5,-297 532.5,-303 532.5,-309 532.5,-309 532.5,-353 532.5,-353 532.5,-359 526.5,-365 520.5,-365\"/>\n<text text-anchor=\"start\" x=\"405\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">thal ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"383.5\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 1.868</text>\n<text text-anchor=\"start\" x=\"386.5\" y=\"-319.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 241</text>\n<text text-anchor=\"start\" x=\"344.5\" y=\"-304.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [128, 45, 27, 30, 11]</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#eca06a\" stroke=\"#000000\" d=\"M409,-261C409,-261 262,-261 262,-261 256,-261 250,-255 250,-249 250,-249 250,-205 250,-205 250,-199 256,-193 262,-193 262,-193 409,-193 409,-193 415,-193 421,-199 421,-205 421,-205 421,-249 421,-249 421,-255 415,-261 409,-261\"/>\n<text text-anchor=\"start\" x=\"309.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">ca ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"284.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 1.098</text>\n<text text-anchor=\"start\" x=\"287.5\" y=\"-215.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 128</text>\n<text text-anchor=\"start\" x=\"258\" y=\"-200.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [100, 16, 5, 6, 1]</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M402.0837,-296.9465C393.4514,-287.8782 384.0289,-277.9799 375.0605,-268.5585\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"377.419,-265.9599 367.9891,-261.13 372.3489,-270.7863 377.419,-265.9599\"/>\n<text text-anchor=\"middle\" x=\"367.3736\" y=\"-282.4224\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#fdfffd\" stroke=\"#000000\" d=\"M615.5,-261C615.5,-261 451.5,-261 451.5,-261 445.5,-261 439.5,-255 439.5,-249 439.5,-249 439.5,-205 439.5,-205 439.5,-199 445.5,-193 451.5,-193 451.5,-193 615.5,-193 615.5,-193 621.5,-193 627.5,-199 627.5,-205 627.5,-205 627.5,-249 627.5,-249 627.5,-255 621.5,-261 615.5,-261\"/>\n<text text-anchor=\"start\" x=\"489.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">oldpeak ≤ 7.0</text>\n<text text-anchor=\"start\" x=\"482.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 2.246</text>\n<text text-anchor=\"start\" x=\"485.5\" y=\"-215.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 113</text>\n<text text-anchor=\"start\" x=\"447.5\" y=\"-200.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [28, 29, 22, 24, 10]</text>\n</g>\n<!-- 0&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>0&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M466.9163,-296.9465C475.5486,-287.8782 484.9711,-277.9799 493.9395,-268.5585\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"496.6511,-270.7863 501.0109,-261.13 491.581,-265.9599 496.6511,-270.7863\"/>\n<text text-anchor=\"middle\" x=\"501.6264\" y=\"-282.4224\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#e78c4b\" stroke=\"#000000\" d=\"M229,-157C229,-157 98,-157 98,-157 92,-157 86,-151 86,-145 86,-145 86,-101 86,-101 86,-95 92,-89 98,-89 98,-89 229,-89 229,-89 235,-89 241,-95 241,-101 241,-101 241,-145 241,-145 241,-151 235,-157 229,-157\"/>\n<text text-anchor=\"start\" x=\"118\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">thalach ≤ 59.5</text>\n<text text-anchor=\"start\" x=\"112.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.463</text>\n<text text-anchor=\"start\" x=\"119.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 84</text>\n<text text-anchor=\"start\" x=\"94\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [77, 6, 1, 0, 0]</text>\n</g>\n<!-- 1&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>1&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M279.1808,-192.9465C262.9952,-183.1599 245.211,-172.4066 228.5448,-162.3294\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"230.3141,-159.3092 219.9458,-157.13 226.6921,-165.2993 230.3141,-159.3092\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#f5cfb3\" stroke=\"#000000\" d=\"M410,-149.5C410,-149.5 271,-149.5 271,-149.5 265,-149.5 259,-143.5 259,-137.5 259,-137.5 259,-108.5 259,-108.5 259,-102.5 265,-96.5 271,-96.5 271,-96.5 410,-96.5 410,-96.5 416,-96.5 422,-102.5 422,-108.5 422,-108.5 422,-137.5 422,-137.5 422,-143.5 416,-149.5 410,-149.5\"/>\n<text text-anchor=\"start\" x=\"289.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 1.806</text>\n<text text-anchor=\"start\" x=\"296.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 44</text>\n<text text-anchor=\"start\" x=\"267\" y=\"-104.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [23, 10, 4, 6, 1]</text>\n</g>\n<!-- 1&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>1&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M337.1372,-192.9465C337.6509,-182.2621 338.2199,-170.4254 338.7416,-159.5742\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"342.2396,-159.6987 339.2239,-149.5422 335.2477,-159.3625 342.2396,-159.6987\"/>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#eb9c63\" stroke=\"#000000\" d=\"M143,-53C143,-53 12,-53 12,-53 6,-53 0,-47 0,-41 0,-41 0,-12 0,-12 0,-6 6,0 12,0 12,0 143,0 143,0 149,0 155,-6 155,-12 155,-12 155,-41 155,-41 155,-47 149,-53 143,-53\"/>\n<text text-anchor=\"start\" x=\"26.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.785</text>\n<text text-anchor=\"start\" x=\"33.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 39</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [32, 6, 1, 0, 0]</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M133.1796,-88.9777C125.019,-79.8207 116.1892,-69.9129 108.0179,-60.744\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"110.6042,-58.3854 101.338,-53.2485 105.3783,-63.0427 110.6042,-58.3854\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#e58139\" stroke=\"#000000\" d=\"M316,-53C316,-53 185,-53 185,-53 179,-53 173,-47 173,-41 173,-41 173,-12 173,-12 173,-6 179,0 185,0 185,0 316,0 316,0 322,0 328,-6 328,-12 328,-12 328,-41 328,-41 328,-47 322,-53 316,-53\"/>\n<text text-anchor=\"start\" x=\"208\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"206.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 45</text>\n<text text-anchor=\"start\" x=\"181\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [45, 0, 0, 0, 0]</text>\n</g>\n<!-- 2&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>2&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M194.1729,-88.9777C202.4285,-79.8207 211.3609,-69.9129 219.6272,-60.744\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.2883,-63.0193 226.3848,-53.2485 217.0892,-58.3321 222.2883,-63.0193\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#f8dcc8\" stroke=\"#000000\" d=\"M599,-149.5C599,-149.5 460,-149.5 460,-149.5 454,-149.5 448,-143.5 448,-137.5 448,-137.5 448,-108.5 448,-108.5 448,-102.5 454,-96.5 460,-96.5 460,-96.5 599,-96.5 599,-96.5 605,-96.5 611,-102.5 611,-108.5 611,-108.5 611,-137.5 611,-137.5 611,-143.5 605,-149.5 599,-149.5\"/>\n<text text-anchor=\"start\" x=\"478.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 1.595</text>\n<text text-anchor=\"start\" x=\"485.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 37</text>\n<text text-anchor=\"start\" x=\"456\" y=\"-104.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [19, 12, 4, 2, 0]</text>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>6&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M532.1903,-192.9465C531.7793,-182.2621 531.3241,-170.4254 530.9067,-159.5742\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"534.4027,-159.4002 530.5209,-149.5422 527.4079,-159.6693 534.4027,-159.4002\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#f2f1fd\" stroke=\"#000000\" d=\"M796,-157C796,-157 641,-157 641,-157 635,-157 629,-151 629,-145 629,-145 629,-101 629,-101 629,-95 635,-89 641,-89 641,-89 796,-89 796,-89 802,-89 808,-95 808,-101 808,-101 808,-145 808,-145 808,-151 802,-157 796,-157\"/>\n<text text-anchor=\"start\" x=\"673\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">thalach ≤ 33.0</text>\n<text text-anchor=\"start\" x=\"667.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 2.243</text>\n<text text-anchor=\"start\" x=\"674.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 76</text>\n<text text-anchor=\"start\" x=\"637\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [9, 17, 18, 22, 10]</text>\n</g>\n<!-- 6&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>6&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M594.0759,-192.9465C611.6445,-183.0701 630.9643,-172.2093 649.0321,-162.0522\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"650.7861,-165.0814 657.788,-157.13 647.3558,-158.9795 650.7861,-165.0814\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#d2d1f9\" stroke=\"#000000\" d=\"M697,-53C697,-53 566,-53 566,-53 560,-53 554,-47 554,-41 554,-41 554,-12 554,-12 554,-6 560,0 566,0 566,0 697,0 697,0 703,0 709,-6 709,-12 709,-12 709,-41 709,-41 709,-47 703,-53 697,-53\"/>\n<text text-anchor=\"start\" x=\"580.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 1.968</text>\n<text text-anchor=\"start\" x=\"587.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 35</text>\n<text text-anchor=\"start\" x=\"562\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 9, 4, 15, 6]</text>\n</g>\n<!-- 8&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>8&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M687.8271,-88.9777C679.5715,-79.8207 670.6391,-69.9129 662.3728,-60.744\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"664.9108,-58.3321 655.6152,-53.2485 659.7117,-63.0193 664.9108,-58.3321\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#dbfaf4\" stroke=\"#000000\" d=\"M870,-53C870,-53 739,-53 739,-53 733,-53 727,-47 727,-41 727,-41 727,-12 727,-12 727,-6 733,0 739,0 739,0 870,0 870,0 876,0 882,-6 882,-12 882,-12 882,-41 882,-41 882,-47 876,-53 870,-53\"/>\n<text text-anchor=\"start\" x=\"753.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 2.212</text>\n<text text-anchor=\"start\" x=\"760.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 41</text>\n<text text-anchor=\"start\" x=\"735\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [8, 8, 14, 7, 4]</text>\n</g>\n<!-- 8&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>8&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M748.8204,-88.9777C756.981,-79.8207 765.8108,-69.9129 773.9821,-60.744\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"776.6217,-63.0427 780.662,-53.2485 771.3958,-58.3854 776.6217,-63.0427\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 223
        }
      ],
      "source": [
        "# Include decision tree visualization here\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", min_weight_fraction_leaf=.1)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "# tree.plot_tree(clf)\n",
        "\n",
        "import graphviz \n",
        "dot_data = tree.export_graphviz(clf, out_file=None, \n",
        "                      feature_names=['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal'],\n",
        "                      filled=True, rounded=True,  \n",
        "                      special_characters=True)  \n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph \n",
        "# Discuss what the model has learned\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Discussion:\n",
        "the model has learned that when 'thal' is less than or equal to 0.5 and ca is greater 0.5 that the classification of patients is 23,10,4,6, or 1. To be honest though, I'm not totally sure what the metadata of this heart disease dataset actually means in terms of the classes. To inspect the metadata and data, go to this link: https://archive.ics.uci.edu/ml/datasets/Heart+Disease"
      ],
      "metadata": {
        "id": "L56fDa5kbmov"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIbsDAyxmuA1"
      },
      "source": [
        "## 6. (optional 5% extra credit) Implement reduced error pruning to help avoid overfitting.  \n",
        "- You will need to take a validation set out of your training data to do this, while still having a test set to test your final accuracy. \n",
        "- Create a table comparing your decision tree implementation's results on the cars and voting data sets with and without reduced error pruning. \n",
        "- This table should compare:\n",
        "    - a) The # of nodes (including leaf nodes) and tree depth of the final decision trees \n",
        "    - b) The generalization (test set) accuracy. (For the unpruned 10-fold CV models, just use their average values in the table)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "lab_3_decision_tree.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}